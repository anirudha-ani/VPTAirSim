{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to make an IDM architecture \n",
    "\n",
    "### Step 1: Load the image dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the images from the data. \n",
    "First getting the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = 'data'\n",
    "data_dir = os.path.abspath(relative_path)\n",
    "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the filenames to load images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(filenames)\n",
    "dataset = torch.zeros(dataset_size, 3, 128, 128)\n",
    "for i in range(dataset_size-4):\n",
    " dataset[i] = torchvision.io.read_image(os.path.join(data_dir, filenames[i]))\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the pixel value by dividing it by 255. Now it is between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.size()\n",
    "dataset = dataset / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Pass the data through 3D Convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a final model. I am writing this to make proper data shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal3DConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Temporal3DConv, self).__init__()\n",
    "\n",
    "        # 3 is input channel because of RGB images. \n",
    "        # 128 is the output channel or learnable filters\n",
    "        # Kernel size 5 is temporal kernel width \n",
    "        # (1*1) is spatial kernel width\n",
    "        # 2 Depth padding for initial and end frames\n",
    "        self.conv3d = nn.Conv3d(3, 128, kernel_size=(5, 1, 1), padding=(2,0,0))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal3DConv = Temporal3DConv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to make 128 size chunks of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset\n",
    "data = TensorDataset(dataset)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(data, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Conv3D the input format is (batch_size, num_channels, num_frames, height, width)\n",
    "So I am using unsqueeze to increase the outer dimension to make batch_size = 1 . \n",
    "\n",
    "Then using the permute to make the dimension in proper shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> torch.Size([1, 128, 3, 128, 128])\n",
      "torch.Size([1, 128, 128, 128, 128])\n",
      ">>> torch.Size([1, 128, 3, 128, 128])\n",
      "torch.Size([1, 128, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataloader in batches\n",
    "output = any\n",
    "for framechunk in dataloader:\n",
    "    # Access the batched tensor data\n",
    "    # Pass the input through the model\n",
    "    print(\">>>\",framechunk[0].unsqueeze(0).size())\n",
    "    output = temporal3DConv(framechunk[0].unsqueeze(0).permute(0, 2, 1, 3, 4) )\n",
    "\n",
    "    print(output.shape)  # Shape of the output tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Pass the 3D Convolution layer outcome through ResNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the middle Res Net layer. So the ResNetBlock represents the Deep residual learning for image\n",
    "recognition paper architecture. \n",
    "\n",
    "ResNetBlocksWithPooling represents the Resnet stack mentioned in the VPT paper. We will use three stacks consecuvely then flatten it before passing it to attention layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResNetBlocksWithPooling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResNetBlocksWithPooling, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.resnet_block1 = ResNetBlock(out_channels, out_channels)\n",
    "        self.resnet_block2 = ResNetBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.resnet_block1(x)\n",
    "        x = self.resnet_block2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a sample code to check if the design checks out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128, 128, 128])\n",
      "torch.Size([128, 128, 128, 128])\n",
      "torch.Size([128, 64, 64, 64])\n",
      "torch.Size([128, 64, 32, 32])\n",
      "torch.Size([128, 64, 16, 16])\n",
      "torch.Size([128, 16384])\n"
     ]
    }
   ],
   "source": [
    "print(output.size())\n",
    "xx = output.permute(0, 2, 1, 3, 4).contiguous().view(1 * 128, 128, 128, 128)\n",
    "print(xx.size())\n",
    "layer = ResNetBlocksWithPooling(128, 64)\n",
    "layer2 = ResNetBlocksWithPooling(64, 64)\n",
    "layer3 = ResNetBlocksWithPooling(64, 64)\n",
    "flattenLayer = nn.Flatten()\n",
    "f = layer.forward(xx)\n",
    "f2 = layer2.forward(f)\n",
    "f3 = layer3.forward(f2)\n",
    "f4 = flattenLayer(f3)\n",
    "print(f.size())\n",
    "print(f2.size())\n",
    "print(f3.size())\n",
    "print(f4.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
