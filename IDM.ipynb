{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to make an IDM architecture \n",
    "\n",
    "### Step 1: Load the image dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the images from the data. \n",
    "First getting the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = 'data'\n",
    "data_dir = os.path.abspath(relative_path)\n",
    "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the filenames to load images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(filenames)\n",
    "dataset = torch.zeros(dataset_size, 3, 128, 128)\n",
    "for i in range(dataset_size-4):\n",
    " dataset[i] = torchvision.io.read_image(os.path.join(data_dir, filenames[i]))\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the pixel value by dividing it by 255. Now it is between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.size()\n",
    "dataset = dataset / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Pass the data through 3D Convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a final model. I am writing this to make proper data shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal3DConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Temporal3DConv, self).__init__()\n",
    "\n",
    "        # 3 is input channel because of RGB images. \n",
    "        # 128 is the output channel or learnable filters\n",
    "        # Kernel size 5 is temporal kernel width \n",
    "        # (1*1) is spatial kernel width\n",
    "        # 2 Depth padding for initial and end frames\n",
    "        self.conv3d = nn.Conv3d(3, 128, kernel_size=(5, 1, 1), padding=(2,0,0))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3d(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal3DConv = Temporal3DConv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to make 128 size chunks of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset\n",
    "data = TensorDataset(dataset)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(data, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Conv3D the input format is (batch_size, num_channels, num_frames, height, width)\n",
    "So I am using unsqueeze to increase the outer dimension to make batch_size = 1 . \n",
    "\n",
    "Then using the permute to make the dimension in proper shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> torch.Size([1, 128, 3, 128, 128])\n",
      "torch.Size([1, 128, 128, 128, 128])\n",
      ">>> torch.Size([1, 128, 3, 128, 128])\n",
      "torch.Size([1, 128, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataloader in batches\n",
    "output = any\n",
    "for framechunk in dataloader:\n",
    "    # Access the batched tensor data\n",
    "    # Pass the input through the model\n",
    "    print(\">>>\",framechunk[0].unsqueeze(0).size())\n",
    "    output = temporal3DConv(framechunk[0].unsqueeze(0).permute(0, 2, 1, 3, 4) )\n",
    "\n",
    "    print(output.shape)  # Shape of the output tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Pass the 3D Convolution layer outcome through ResNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the middle Res Net layer. So the ResNetBlock represents the Deep residual learning for image\n",
    "recognition paper architecture. \n",
    "\n",
    "ResNetBlocksWithPooling represents the Resnet stack mentioned in the VPT paper. We will use three stacks consecuvely then flatten it before passing it to attention layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResNetBlocksWithPooling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResNetBlocksWithPooling, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.resnet_block1 = ResNetBlock(out_channels, out_channels)\n",
    "        self.resnet_block2 = ResNetBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.resnet_block1(x)\n",
    "        x = self.resnet_block2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a sample code to check if the design checks out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128, 128, 128])\n",
      "torch.Size([128, 128, 128, 128])\n",
      "torch.Size([128, 64, 64, 64])\n",
      "torch.Size([128, 64, 32, 32])\n",
      "torch.Size([128, 64, 16, 16])\n",
      "torch.Size([128, 16384])\n"
     ]
    }
   ],
   "source": [
    "print(output.size())\n",
    "# To match the expected input shape of the ResNet model, we need to reshape the output tensor. \n",
    "# First, we use permute to rearrange the dimensions of the tensor, swapping the second and third dimensions. \n",
    "# Then, we use contiguous to ensure the tensor's memory is laid out contiguously. \n",
    "# Finally, we use view to reshape the tensor into a 4D tensor with dimensions (batch_size * num_frames, num_channels, height, width).\n",
    "xx = output.permute(0, 2, 1, 3, 4).contiguous().view(1 * 128, 128, 128, 128)\n",
    "print(xx.size())\n",
    "layer = ResNetBlocksWithPooling(128, 64)\n",
    "layer2 = ResNetBlocksWithPooling(64, 64)\n",
    "layer3 = ResNetBlocksWithPooling(64, 64)\n",
    "flattenLayer = nn.Flatten()\n",
    "f = layer.forward(xx)\n",
    "f2 = layer2.forward(f)\n",
    "f3 = layer3.forward(f2)\n",
    "f4 = flattenLayer(f3)\n",
    "print(f.size())\n",
    "print(f2.size())\n",
    "print(f3.size())\n",
    "print(f4.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Pass ResNet outcome through Multiheaded Residual Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FrameWiseDense(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(FrameWiseDense, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResidualTransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout=0.1):\n",
    "        super(ResidualTransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.dense1 = FrameWiseDense(embedding_dim, 16384)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dense2 = FrameWiseDense(16384, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out, _ = self.attention(x, x, x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.norm1(out + residual)\n",
    "        residual = out\n",
    "        out = self.dense1(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.dense2(out)\n",
    "        out = self.norm2(out + residual)\n",
    "        return out\n",
    "\n",
    "class ActionPredictionModel(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(ActionPredictionModel, self).__init__()\n",
    "        # the initial value is 16384 because it is the flattened output dimension for ResNet\n",
    "        self.dense1 = FrameWiseDense(16384, 256)\n",
    "        self.dense2 = FrameWiseDense(256, 4096)\n",
    "        self.residual_transformer_blocks = nn.Sequential(\n",
    "            ResidualTransformerBlock(embedding_dim=4096, num_heads=32),\n",
    "            ResidualTransformerBlock(embedding_dim=4096, num_heads=32),\n",
    "            ResidualTransformerBlock(embedding_dim=4096, num_heads=32),\n",
    "            ResidualTransformerBlock(embedding_dim=4096, num_heads=32)\n",
    "        )\n",
    "        self.dense3 = FrameWiseDense(4096, 16384)\n",
    "        self.dense4 = FrameWiseDense(16384, 4096)\n",
    "        self.action_head = nn.Linear(4096, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dense1(x)\n",
    "        out = self.dense2(out)\n",
    "        print(out.size())\n",
    "        # out = out.permute(1, 0, 2)\n",
    "        out = self.residual_transformer_blocks(out)\n",
    "        # out = out.permute(1, 0, 2)\n",
    "        out = self.dense3(out)\n",
    "        out = self.dense4(out)\n",
    "        print(out.size())\n",
    "        # out = out.mean(dim=1)\n",
    "        out = self.action_head(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy code to check model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4096])\n",
      "torch.Size([128, 4096])\n",
      "tensor([[0.2615, 0.2518, 0.2016, 0.2851],\n",
      "        [0.2600, 0.2561, 0.1945, 0.2894],\n",
      "        [0.2632, 0.2460, 0.1998, 0.2911],\n",
      "        [0.2584, 0.2536, 0.1964, 0.2916],\n",
      "        [0.2614, 0.2594, 0.1989, 0.2803],\n",
      "        [0.2647, 0.2504, 0.1989, 0.2860],\n",
      "        [0.2624, 0.2554, 0.1919, 0.2903],\n",
      "        [0.2511, 0.2613, 0.1989, 0.2887],\n",
      "        [0.2591, 0.2571, 0.1968, 0.2869],\n",
      "        [0.2695, 0.2571, 0.1995, 0.2739],\n",
      "        [0.2523, 0.2622, 0.1965, 0.2890],\n",
      "        [0.2689, 0.2510, 0.1986, 0.2815],\n",
      "        [0.2542, 0.2593, 0.2032, 0.2833],\n",
      "        [0.2609, 0.2476, 0.2034, 0.2881],\n",
      "        [0.2605, 0.2552, 0.1984, 0.2860],\n",
      "        [0.2540, 0.2559, 0.2041, 0.2859],\n",
      "        [0.2612, 0.2537, 0.2023, 0.2829],\n",
      "        [0.2674, 0.2513, 0.2000, 0.2814],\n",
      "        [0.2570, 0.2571, 0.2025, 0.2834],\n",
      "        [0.2679, 0.2579, 0.1928, 0.2814],\n",
      "        [0.2603, 0.2495, 0.2016, 0.2887],\n",
      "        [0.2669, 0.2479, 0.1948, 0.2904],\n",
      "        [0.2562, 0.2514, 0.2015, 0.2910],\n",
      "        [0.2559, 0.2567, 0.1950, 0.2924],\n",
      "        [0.2603, 0.2589, 0.1940, 0.2868],\n",
      "        [0.2589, 0.2547, 0.2020, 0.2844],\n",
      "        [0.2610, 0.2532, 0.2003, 0.2855],\n",
      "        [0.2672, 0.2528, 0.1957, 0.2843],\n",
      "        [0.2592, 0.2529, 0.1983, 0.2897],\n",
      "        [0.2563, 0.2540, 0.2026, 0.2871],\n",
      "        [0.2563, 0.2568, 0.1991, 0.2879],\n",
      "        [0.2638, 0.2593, 0.1955, 0.2814],\n",
      "        [0.2614, 0.2500, 0.1940, 0.2946],\n",
      "        [0.2613, 0.2565, 0.1976, 0.2846],\n",
      "        [0.2620, 0.2431, 0.2058, 0.2890],\n",
      "        [0.2592, 0.2567, 0.1991, 0.2850],\n",
      "        [0.2610, 0.2460, 0.1973, 0.2957],\n",
      "        [0.2589, 0.2508, 0.2023, 0.2879],\n",
      "        [0.2594, 0.2413, 0.2033, 0.2960],\n",
      "        [0.2605, 0.2567, 0.1986, 0.2842],\n",
      "        [0.2595, 0.2494, 0.2013, 0.2897],\n",
      "        [0.2594, 0.2534, 0.2039, 0.2833],\n",
      "        [0.2597, 0.2514, 0.2037, 0.2852],\n",
      "        [0.2646, 0.2542, 0.1966, 0.2845],\n",
      "        [0.2592, 0.2527, 0.1990, 0.2891],\n",
      "        [0.2557, 0.2558, 0.1994, 0.2892],\n",
      "        [0.2635, 0.2470, 0.2031, 0.2865],\n",
      "        [0.2478, 0.2659, 0.1945, 0.2919],\n",
      "        [0.2638, 0.2480, 0.1985, 0.2897],\n",
      "        [0.2551, 0.2637, 0.1919, 0.2893],\n",
      "        [0.2555, 0.2575, 0.2015, 0.2854],\n",
      "        [0.2666, 0.2465, 0.1951, 0.2918],\n",
      "        [0.2617, 0.2618, 0.1964, 0.2801],\n",
      "        [0.2662, 0.2525, 0.1998, 0.2815],\n",
      "        [0.2583, 0.2558, 0.1941, 0.2917],\n",
      "        [0.2546, 0.2559, 0.1999, 0.2896],\n",
      "        [0.2582, 0.2547, 0.1987, 0.2883],\n",
      "        [0.2617, 0.2547, 0.2000, 0.2836],\n",
      "        [0.2560, 0.2585, 0.2036, 0.2818],\n",
      "        [0.2553, 0.2537, 0.2021, 0.2889],\n",
      "        [0.2541, 0.2547, 0.2061, 0.2851],\n",
      "        [0.2593, 0.2553, 0.1944, 0.2911],\n",
      "        [0.2629, 0.2442, 0.2000, 0.2929],\n",
      "        [0.2652, 0.2562, 0.1961, 0.2826],\n",
      "        [0.2598, 0.2464, 0.1960, 0.2978],\n",
      "        [0.2626, 0.2482, 0.1986, 0.2905],\n",
      "        [0.2679, 0.2448, 0.1884, 0.2989],\n",
      "        [0.2687, 0.2467, 0.1982, 0.2864],\n",
      "        [0.2578, 0.2615, 0.1970, 0.2837],\n",
      "        [0.2548, 0.2520, 0.1975, 0.2956],\n",
      "        [0.2582, 0.2577, 0.1971, 0.2870],\n",
      "        [0.2535, 0.2589, 0.1986, 0.2889],\n",
      "        [0.2611, 0.2549, 0.1963, 0.2878],\n",
      "        [0.2576, 0.2545, 0.1996, 0.2883],\n",
      "        [0.2596, 0.2511, 0.2024, 0.2869],\n",
      "        [0.2601, 0.2572, 0.2028, 0.2799],\n",
      "        [0.2498, 0.2559, 0.2013, 0.2930],\n",
      "        [0.2560, 0.2559, 0.1996, 0.2885],\n",
      "        [0.2644, 0.2487, 0.2015, 0.2855],\n",
      "        [0.2592, 0.2586, 0.1951, 0.2871],\n",
      "        [0.2640, 0.2589, 0.1945, 0.2826],\n",
      "        [0.2491, 0.2546, 0.2046, 0.2917],\n",
      "        [0.2550, 0.2533, 0.1994, 0.2922],\n",
      "        [0.2582, 0.2548, 0.2034, 0.2836],\n",
      "        [0.2576, 0.2585, 0.1965, 0.2875],\n",
      "        [0.2596, 0.2518, 0.1933, 0.2953],\n",
      "        [0.2646, 0.2538, 0.1954, 0.2862],\n",
      "        [0.2580, 0.2477, 0.2020, 0.2924],\n",
      "        [0.2599, 0.2537, 0.1997, 0.2867],\n",
      "        [0.2562, 0.2559, 0.1999, 0.2880],\n",
      "        [0.2652, 0.2517, 0.1999, 0.2832],\n",
      "        [0.2671, 0.2508, 0.2008, 0.2812],\n",
      "        [0.2713, 0.2574, 0.1959, 0.2754],\n",
      "        [0.2595, 0.2527, 0.1967, 0.2911],\n",
      "        [0.2573, 0.2522, 0.2051, 0.2854],\n",
      "        [0.2559, 0.2620, 0.1989, 0.2832],\n",
      "        [0.2656, 0.2466, 0.2006, 0.2872],\n",
      "        [0.2562, 0.2534, 0.2056, 0.2847],\n",
      "        [0.2621, 0.2446, 0.2059, 0.2874],\n",
      "        [0.2603, 0.2480, 0.1951, 0.2967],\n",
      "        [0.2641, 0.2464, 0.1936, 0.2959],\n",
      "        [0.2608, 0.2554, 0.1997, 0.2841],\n",
      "        [0.2641, 0.2545, 0.1988, 0.2826],\n",
      "        [0.2615, 0.2559, 0.1971, 0.2855],\n",
      "        [0.2643, 0.2563, 0.1942, 0.2852],\n",
      "        [0.2617, 0.2526, 0.1996, 0.2861],\n",
      "        [0.2610, 0.2525, 0.2012, 0.2852],\n",
      "        [0.2586, 0.2549, 0.1983, 0.2882],\n",
      "        [0.2637, 0.2516, 0.1993, 0.2854],\n",
      "        [0.2646, 0.2509, 0.2035, 0.2810],\n",
      "        [0.2582, 0.2543, 0.2015, 0.2860],\n",
      "        [0.2642, 0.2482, 0.2008, 0.2868],\n",
      "        [0.2501, 0.2497, 0.2057, 0.2945],\n",
      "        [0.2544, 0.2539, 0.2009, 0.2908],\n",
      "        [0.2636, 0.2507, 0.1926, 0.2931],\n",
      "        [0.2577, 0.2527, 0.2023, 0.2872],\n",
      "        [0.2528, 0.2579, 0.1961, 0.2933],\n",
      "        [0.2550, 0.2576, 0.1982, 0.2892],\n",
      "        [0.2698, 0.2402, 0.2020, 0.2881],\n",
      "        [0.2710, 0.2525, 0.1898, 0.2867],\n",
      "        [0.2581, 0.2537, 0.2014, 0.2868],\n",
      "        [0.2533, 0.2616, 0.2014, 0.2838],\n",
      "        [0.2568, 0.2601, 0.1949, 0.2882],\n",
      "        [0.2557, 0.2579, 0.1952, 0.2911],\n",
      "        [0.2601, 0.2507, 0.1986, 0.2907],\n",
      "        [0.2662, 0.2507, 0.1903, 0.2928],\n",
      "        [0.2538, 0.2522, 0.2004, 0.2936],\n",
      "        [0.2593, 0.2536, 0.1962, 0.2909]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the model\n",
    "model = ActionPredictionModel(num_actions=4)\n",
    "\n",
    "output = model(f4)\n",
    "print(output)  # Output shape: (128, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
